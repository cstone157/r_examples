---
Machine Learning with R
Chapter 07 - Black Box Methods - Neural Networks and Support Vector Machines
---

  - <b>black box</b> - a process that transorms the input into the output is obfuscated by an imaginary box.

## Understanding neural networks

  - <b>artificial neural network (ANN)</b> : models the relationship between a set of input signals and an output signal using a model derived from how a biological brain works.
  - <b>neurons</b> : interconnected class that form a network, to make a brain.
  - <b>nodes</b> : artificial neurons, that forms a ANN.
  - <b>Turing test</b> : controversial question, proposed in 1950 by Alan Turing, which grades a machines intelligence, based on if a human can distinguish it's behavior from a living creature's.
  - Use cases
    - Speech, handwriting, and image recognition
    - Automation of smart devices
    - Sophisticated models of weather and climate patterns, tensile strength, fluid dynamics, ecetera.

### From biological to artificial neurons

  - Biological system to be modeled
    - <b>Dendrites</b> : part of the cell that receiving incoming signals
    - <b>Cell body</b> : accumulates the incoming signals, a threshold is readed at which point the cell fires
    - <b>Axon</b> : transmit the electrochemical output signal
    - <b>Synapse</b> : the chemical signal passed to neighboring neurons across a tiny gap after the electric signal is processed
  - Single artificial neuron
    - <b>Activation function</b> : transforms a neuron's net input signal into a single output signal to be broadcasted further in the network
    - <b>Network topology</b> : describes the number of neurons in the model as well as the number of layers and manner in which they are connected.
    - <b>Training algorithm</b> : specifies how connection weights are set in order to inhibit to excite neurons in proportion to the input signal.

### Activation functions

  - The activation function is the mechanism by which the artificial neuron processes incoming information and passes it throughout the network.
  - <b>Threshold activation function</b> : summing the total input signal and determining whether it meets the firing threshold.
    - Threshold activation function is also sometimes known as <b>unit step activation function</b>
  - <b>Sigmoid activation function</b> : most commonly used activation function.
    - f(x) = 1 / (1 + e^(-x))
    - The sigmoid is differential: meaning it is possible to calculate the derivative across entire range of inputs.
  - Gaussian activation function is the basis of a <b>radial basis function (RBF) network</b>.
  - Because activation functions have a small range of outputs, activation function like sigmoid are sometimes called <b>squashing functions</b>.

### Network topology
  - <b>Topology</b> : pattern and structures of interconnected neurons.
    - Number of layers
    - information in the network is allowed to travel backward
    - number of nodes within each layer

### The number of layers

  - A set of neurons called <b>input nodes</b> receives unprocessed signals directly from the input data.
  - The signals sent by input nodes are received by the <b>output node</b> which uses its own activation function to generate a final predictions
  - The input and output nodes are arranged in groups known as <b>layers</b>.
  - <b>Single-layer network</b> : one set of connection weights.  Used for basic pattern classification, particularly for patterns that are linearly separable
  - <b>multi-layer network</b> : network with more than one layer.
  - <b>hidden layers</b> : layers in the middle of the network.
  - <b>fully connected</b> every node in one layer is connected to every node in the next layer.

### The direction of information travel

  - <b>Feed-forward network</b> : the input signal is fed continuously in one direction from the input layer to the output layer
  - <b>Deep neural network (DNN)</b> : neural network with multiple hidden layers
  - <b>Deep learning</b> : the practice of training deep neural networks.
  - <b>Recurrent network or feedback network</b> : allows signals to travel backwards using loops.
  - <b>Delay</b> : short-term memory
  - <b>Multi-layer perceptron (MLP)</b> : multi-layer feed-forward network

### The number of nodes in each layer

  - There is no reliable rule to determine the number of neurons in hidden layer.

### Training neural networks with back-propagation

  - <b>back-propagation</b> : strategy of back-propagating errors
  - ANN strengths and weaknesses
    - Strengths
      - Can be adapted to classification or numeric prediction problems
      - Capable of modeling more complex patterns than nearly any other algorithm
      - Makes few assumptions about the data's underlying relationships
    - Weaknesses
      - Extremely computationally intensive and slow to train
      - Very prone to overfitting training data
      - Results in a complex black box model that is dificult, if not impossible to interpret
      

## Example - modeling the strength of concrete with ANNs

### Step 1 - collection data

  - Using compressive strength of concrete data from UCI Machine Learning Repository (http://archive.ics.uci.edu/ml).

### Step 2 - exploring and preparing the data
```{r}
# - Read the csv
concrete <- read.csv("concrete.csv")
str(concrete)

# - Define our own normalize function
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# - Apply our normalize function to ever column in the concrete data frame using lapply
concrete_norm <- as.data.frame(lapply(concrete, normalize))

# - Confirm normalization worked
summary(concrete_norm$strength)

# - Compare to original
summary(concrete$strength)

# - Divide our data into training (75%) and testing sets (25%)
concrete_train <- concrete_norm[1:773, ]
concrete_test <- concrete_norm[774:1030, ]
```

### Step 3 - training a model on data
```{r}
# - We're going to use the neuralnet package.
#install.packages("neuralnet")
library(neuralnet)

# - Neural network syntax
#   - Building the model:
#     - m <- neuralnet(target ~ predictors, data = mydata, hidden = 1, act.fct = "logistic")
#     - target is the outcome of the mydata data frame to be modeled
#     - predictors is an R formula specifying the features in the mydata data frame to use for prediction
#     - data specifies the data frame where the target and predictors are found
#     - hidden specifies the number of neurons in the hidden layer (by default 1).  Note: use an integer vector to specify multiple hidden layers, for example c(2, 2)
#     - act.fct specifies the activation function, either "logistic" or "tanh".  Note: a differentiable custom activation function can also be supplied.
#   - Making predictions: 
#     - p <- compute(m, test)
#     - m is a model trained by the neuralnet() function
#     - test is a data frame containing test data with the same features as the training data used to build the classifier

concrete_model <- neuralnet(strength ~ cement + slag + ash + water + 
                              superplastic + coarseagg + fineagg + age, 
                            data = concrete_train)

# - We can visualize the network topology using plot() function
#plot(concrete_model)
plot(concrete_model, rep = "best")

# - At the bottom of the figure, R reports the number of training steps and an error 
#     measure called the sum of squared errors (SSE).  Is the sum of the squared 
#     differences between the predicted and actual values.
```

### Step 4 - evaluating model performance
```{r}
# - Generate predictions on the test data, we can use the compute() function as follows:
model_results <- compute(concrete_model, concrete_test[1:8])

# - Compute() works differently from predict() functions.  Return a list of two 
#     components: $ neurons, which stores the neurons for each layer in the network,
#     and $net.result, which stores the predicted values
predicted_strength <- model_results$net.result

# - Measure the correlation between our predicted concrete strength and the true value.
cor(predicted_strength, concrete_test$strength)
```

### Step 5 - improving model performance
```{r}
# - Increase the number of hidden layers to see if that helps
concrete_model2 <- neuralnet(strength ~ cement + slag + ash + water + 
                            superplastic + coarseagg + fineagg + age, 
                            hidden = 1,
                            data = concrete_train)

#plot(concrete_model2)
plot(concrete_model2, rep = "best")

# - Test the results
model_results2 <- compute(concrete_model2, concrete_test[1:8])
predicted_strength2 <- model_results2$net.result
cor(predicted_strength2, concrete_test$strength)

# - In addition to hidden layers, we can also change the network's activation function.
# - Activation functions
#   - Rectifier : a neural network uses the rectifier activation function known as rectified linear unit (ReLU)
#     - Cannot be used with neuralnet, since it's derivative is undefined at x = 0
#   - Softpulus or SmoothReLU - 
softplus <- function(x) { log(1 + exp(x)) }

set.seed(12345)
concrete_model3 <- neuralnet(strength ~ cement + slag + ash + water + 
                            superplastic + coarseagg + fineagg + age, 
                            hidden = c(5, 5),
                            act.fct = softplus,
                            data = concrete_train)

#plot(concrete_model3)
plot(concrete_model3, rep = "best")

# - Test the results
model_results3 <- compute(concrete_model3, concrete_test[1:8])
predicted_strength3 <- model_results3$net.result
cor(predicted_strength3, concrete_test$strength)

# - Since we normalized the data prior to training, the predictions are also on a 
#   normalized scale.
strengths <- data.frame(
  actual = concrete$strength[774:1030],
  pred = predicted_strength3
)

head(strengths, n = 3)

# Check the correclation between predicted and actual
cor(strengths$pred, strengths$actual)

# Unnormalize our data
unnormalize <- function(x) {
  return ((x * (max(concrete$strength)) - 
             min(concrete$strength)) + min(concrete$strength))
}

strengths$pred_new <- unnormalize(strengths$pred)
strengths$error <- strengths$pred_new - strengths$actual

head(strengths, n = 3)

cor(strengths$pred_new, strengths$actual)

# Vanishing gradient problem and exploding gradient problem: where the backpropagation
#   algorithm fails to find a useful solution due to an inability to converge in a reasonable time.
```

### Understanding support vector machines

  - <b>support vector machine (SVM)</b> : imagined as a surface that creates a boundary between points of data plotted in a multidimensional space representing examples of their feature values.
  - <b>hyperplane</b> : divides the space to create faily homogeneous partitions on either side.
  - SVM learning combines aspects of both the instance-base nearest neighbor learning presented in CH03 and linear regression modeling in CH06.
  - Use cases:
    - Classification of microarray gene expression
    - text categorization
    - detection of rate yet important events

### Classification with hyperplanes

  - If a straight line or flat surface can separate classes, they are said to be <b>linearly separable</b>.
  - <b>maximum margin hyperplane (MMH)</b> : creates the greatest separation between two classes.
  - <b>support vectors</b> : are the points from each class that are the closet to the MMH.  Each class must have at least one support vector.

### The case of linearly sparable data

  - <b>convex hull</b> : the outer boundaries of a MMH
  - <b>quadratic optimization</b> : algorithms  that use calculate a MMH is perpendicular bisector of the shortest line between two convex hulls.
  - alternative approach involves a search through the space of every possible hyperplane in order to find a set of two parallel planes that divide the points into homogeneous groups yet themselves are as far apart as possible.
  - <b>Euclidean norm</b> : (||w||) the distance from the origin to vector w.

### The case of nonlinearly separable data

  - <b>stack variable</b> : when data is not linearly separable, creates a soft margin that allows some points to fall on the incorrect side of the margin.

### Using kernels for nonlinear spaces

  - A key feature of SVMs is their ability to map the problem into a higher dimension space using a process known as <b>kernel trick</b>.
    - With the additions of features, classes that weren't linearly separable, can be linearly separated.
  - SVM with nonlinear kernels are extremely powerful classifiers:
    - Strengths
      - Can be used for classification or numeric predictions
      - Not overly influenced by noisy data and not very prone to overfitting
      - May be easier to use than neural networks
      - Gain popularity due to their high accuracy and high-profile wines in data mining
    - Weaknesses
      - Finding the best model requires the test of various combinations of kernels and model parameters
      - Can be slow to train
      - Results in a complex black box model
  - <b>dot product</b> takes two vectors and returns a single number
  - <b>linear kernel</b> does not transform the data at all.
  - <b>polynomial kernel</b> add a degree d to a simple nonlinear transformation of data
  - <b>sigmoid kernel</b> add a sigmoid activation function
  - <b>Gaussian RBF kernel</b> similar to an RBF neural network.

### Example - performing OCR with SVMs

  - Develop a model similar to those used at core of <b>optical character recognition (OCR)</b>

### Step 1 - collection data

  - <b>glyph</b> : term refers to a letter, symbol or number.
  - First step of OCR processing document is to divide the paper into a matrix such that each cell in the grid contains a single glyph.
  - Use a dataset donated to UCI Machine Learning Repository, containing 20000 examples of 26 English alphabet capital letters

### Step 2 - exploring and preparing the data
```{r}
#letters <- read.csv("letterdata.csv")
# Needed to convert the strings to categorical factors
letters <- read.csv("letterdata.csv", stringsAsFactors = TRUE)
str(letters)

# Split our data
letters_train <- letters[1:16000, ]
letters_test <- letters[16001:20000, ]
```

### Step 3 - training a model on the data
```{r}
# - Support vector machine syntax
#   - Building the model
#     - m <- ksvm(target ~ predictors, data = mydata,
#                 kernel = "rbfdot", c = 1)
#     - target is the outcome in the mydata data frame to be modeled
#     - predictors is an R formula specifying the features in the mydata data frame to use for prediction
#     - data specifies the data frame in which the target and predictors variables can be found
#     - kernel specifies a nonlinear mapping such as "rbfdot" (radial basis), 
#       "polydot" (polynomial), "tanhdot" (hyperbolic tangent sigmoid), or
#       "vanilladog" (linear)
#     - c is a number that specifies the cost of violating the constraints, i.e. how big of a
#       penalty there is for the "soft margin."  Larger values will result in narrower margins
#   - Making predictions:
#     - p <- predict(m, test, type = "response")
#     - m is the model trained by the ksvm() function
#     - test is a data frame containing test data with the same features as the training 
#       data used to build the classifier
#     - type specifies whether the predictions should be "response" (the 
#       predicted class) or "probabilities" (the predicted probability, one 
#       column per class level)
#install.packages("kernlab")
library(kernlab)
letter_classifier <- ksvm(letter ~ ., data = letters_train,
                          kernel = "vanilladot")
# See some basic information about our model
letter_classifier
```

### Step 4 - evaluating model performance
```{r}
# The default type is "response"
letter_predictions <- predict(letter_classifier, letters_test)
head(letter_predictions)

# We'll use the table() function to evalue the predicted letter to the true letter
table(letter_predictions, letters_test$letter)

# create a vector containing TRUE OR FALSE values indicating whether the models predicted
#   letter agrees
agreement <- letter_predictions == letters_test$letter
table(agreement)

# In percentage terms
prop.table(table(agreement))
```

## Step 5 - improving model performance

### Changing the SVM kernel function
```{r}
# Train a RBF-based SVM
letter_classifier_rbf <- ksvm(letter ~ ., data = letters_train,
                              kernel = "rbfdot")

# Make predictions
letter_predictions_rbf <- predict(letter_classifier_rbf, letters_test)

# Compare our accuracy
agreement_rbf <- letter_predictions_rbf == letters_test$letter
table(agreement_rbf)

# In percentage terms
prop.table(table(agreement))
```

### Identifying the best SVM cost parameter
```{r}
# Vary the cost parameter, which modifies the width of the SVM decision boundary.

# There is no rule of thub to know the ideal value beforehand, use sapply() to
#   apply a custom function to vector of potential cost estimates.
cost_values <- c(1, seq(from = 5, to = 40, by = 5))

accuray_values <- sapply(cost_values, function(x) {
  set.seed(12345)
  m <- ksvm(letter ~ ., data = letters_train,
            kernel = "rbfdot", C = x)
  pred <- predict(m, letters_test)
  agree <- ifelse(pred == letters_test$letter, 1, 0)
  accuracy <- sum(agree) / nrow(letters_test)
  return (accuracy)
})

plot(cost_values, accuray_values, type="b")
```