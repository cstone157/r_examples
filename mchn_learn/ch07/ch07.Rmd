---
Machine Learning with R
Chapter 07 - Black Box Methods - Neural Networks and Support Vector Machines
---

  - <b>black box</b> - a process that transorms the input into the output is obfuscated by an imaginary box.

## Understanding neural networks

  - <b>artificial neural network (ANN)</b> : models the relationship between a set of input signals and an output signal using a model derived from how a biological brain works.
  - <b>neurons</b> : interconnected class that form a network, to make a brain.
  - <b>nodes</b> : artificial neurons, that forms a ANN.
  - <b>Turing test</b> : controversial question, proposed in 1950 by Alan Turing, which grades a machines intelligence, based on if a human can distinguish it's behavior from a living creature's.
  - Use cases
    - Speech, handwriting, and image recognition
    - Automation of smart devices
    - Sophisticated models of weather and climate patterns, tensile strength, fluid dynamics, ecetera.

### From biological to artificial neurons

  - Biological system to be modeled
    - <b>Dendrites</b> : part of the cell that receiving incoming signals
    - <b>Cell body</b> : accumulates the incoming signals, a threshold is readed at which point the cell fires
    - <b>Axon</b> : transmit the electrochemical output signal
    - <b>Synapse</b> : the chemical signal passed to neighboring neurons across a tiny gap after the electric signal is processed
  - Single artificial neuron
    - <b>Activation function</b> : transforms a neuron's net input signal into a single output signal to be broadcasted further in the network
    - <b>Network topology</b> : describes the number of neurons in the model as well as the number of layers and manner in which they are connected.
    - <b>Training algorithm</b> : specifies how connection weights are set in order to inhibit to excite neurons in proportion to the input signal.

### Activation functions

  - The activation function is the mechanism by which the artificial neuron processes incoming information and passes it throughout the network.
  - <b>Threshold activation function</b> : summing the total input signal and determining whether it meets the firing threshold.
    - Threshold activation function is also sometimes known as <b>unit step activation function</b>
  - <b>Sigmoid activation function</b> : most commonly used activation function.
    - f(x) = 1 / (1 + e^(-x))
    - The sigmoid is differential: meaning it is possible to calculate the derivative across entire range of inputs.
  - Gaussian activation function is the basis of a <b>radial basis function (RBF) network</b>.
  - Because activation functions have a small range of outputs, activation function like sigmoid are sometimes called <b>squashing functions</b>.

### Network topology
  - <b>Topology</b> : pattern and structures of interconnected neurons.
    - Number of layers
    - information in the network is allowed to travel backward
    - number of nodes within each layer

### The number of layers

  - A set of neurons called <b>input nodes</b> receives unprocessed signals directly from the input data.
  - The signals sent by input nodes are received by the <b>output node</b> which uses its own activation function to generate a final predictions
  - The input and output nodes are arranged in groups known as <b>layers</b>.
  - <b>Single-layer network</b> : one set of connection weights.  Used for basic pattern classification, particularly for patterns that are linearly separable
  - <b>multi-layer network</b> : network with more than one layer.
  - <b>hidden layers</b> : layers in the middle of the network.
  - <b>fully connected</b> every node in one layer is connected to every node in the next layer.

### The direction of information travel

  - <b>Feed-forward network</b> : the input signal is fed continuously in one direction from the input layer to the output layer
  - <b>Deep neural network (DNN)</b> : neural network with multiple hidden layers
  - <b>Deep learning</b> : the practice of training deep neural networks.
  - <b>Recurrent network or feedback network</b> : allows signals to travel backwards using loops.
  - <b>Delay</b> : short-term memory
  - <b>Multi-layer perceptron (MLP)</b> : multi-layer feed-forward network

### The number of nodes in each layer

  - There is no reliable rule to determine the number of neurons in hidden layer.

### Training neural networks with back-propagation

  - <b>back-propagation</b> : strategy of back-propagating errors
  - ANN strengths and weaknesses
    - Strengths
      - Can be adapted to classification or numeric prediction problems
      - Capable of modeling more complex patterns than nearly any other algorithm
      - Makes few assumptions about the data's underlying relationships
    - Weaknesses
      - Extremely computationally intensive and slow to train
      - Very prone to overfitting training data
      - Results in a complex black box model that is dificult, if not impossible to interpret
      

## Example - modeling the strength of concrete with ANNs

### Step 1 - collection data

  - Using compressive strength of concrete data from UCI Machine Learning Repository (http://archive.ics.uci.edu/ml).

### Step 2 - exploring and preparing the data
```{r}
# - Read the csv
concrete <- read.csv("concrete.csv")
str(concrete)

# - Define our own normalize function
normalize <- function(x) {
  return (x - min(x)) / (max(x) - min(x))
}

# - Apply our normalize function to ever column in the concrete data frame using lapply
concrete_norm <- as.data.frame(lapply(concrete, normalize))

# - Confirm normalization worked
summary(concrete_norm$strength)

# - Compare to original
summary(concrete$strength)

# - Divide our data into training (75%) and testing sets (25%)
concrete_train <- concrete_norm[1:773, ]
concrete_test <- concrete_norm[774:1030, ]
```

### Step 3 - training a model on data
```{r}
# - We're going to use the neuralnet package.
#install.packages("neuralnet")
library(neuralnet)

# - Neural network syntax
#   - Building the model:
#     - m <- neuralnet(target ~ predictors, data = mydata, hidden = 1, act.fct = "logistic")
#     - target is the outcome of the mydata data frame to be modeled
#     - predictors is an R formula specifying the features in the mydata data frame to use for prediction
#     - data specifies the data frame where the target and predictors are found
#     - hidden specifies the number of neurons in the hidden layer (by default 1).  Note: use an integer vector to specify multiple hidden layers, for example c(2, 2)
#     - act.fct specifies the activation function, either "logistic" or "tanh".  Note: a differentiable custom activation function can also be supplied.
#   - Making predictions: 
#     - p <- compute(m, test)
#     - m is a model trained by the neuralnet() function
#     - test is a data frame containing test data with the same features as the training data used to build the classifier

concrete_model <- neuralnet(strength ~ cement + slag + ash + water + 
                              superplastic + coarseagg + fineagg + age, 
                            data = concrete_train)

# - We can visualize the network topology using plot() function
plot(concrete_model)

# - At the bottom of the figure, R reports the number of training steps and an error 
#     measure called the sum of squared errors (SSE).  Is the sum of the squared 
#     differences between the predicted and actual values.
```

### Step 4 - evaluating model performance
```{r}
# - Generate predictions on the test data, we can use the compute() function as follows:
model_results <- compute(concrete_model, concrete_test[1:8])

# - Compute() works differently from predict() functions.  Return a list of two 
#     components: $ neurons, which stores the neurons for each layer in the network,
#     and $net.result, which stores the predicted values
predicted_strength <- model_results$net.result

# - Measure the correlation between our predicted concrete strength and the true value.
cor(predicted_strength, concrete_test$strength)

## - TO-DO / ERROR
## - Something is very wrong with this code.  Moving on and gonna try the next 
##    one since this seems to be over-fitting
```

### Step 5 - improving model performance
```{r}
# - Increase the number of hidden layers to see if that helps
concrete_model2 <- neuralnet(strength ~ cement + slag + ash + water + 
                            superplastic + coarseagg + fineagg + age, 
                            hidden = 5,
                            data = concrete_train)

#plot(concrete_model2)

# - Test the results
model_results2 <- compute(concrete_model2, concrete_test[1:8])
predicted_strength2 <- model_results2$net.result
cor(predicted_strength2, concrete_test$strength)
```

### 
  - 


### 
```{r}
```

