---
Machine Learning with R
Chapter 05 - Divide and Conquer - Classification Using Decision Trees and Rules
---

## Understanding decision trees

  - Decision tree learners are powerful classifiers that utilize a tree structure to model the relationships among the features and the potential outcomes.
  - Bottom of a decision tree is a root node, event travels through decision nodes that require choices to be made base on the attributes of the event.
  - The choice split the data across branches that indicate potential outcomes of a decision.
  - If a final decision can be made, the tree is terminated by leaf nodes (also known as terminal nodes).
  - One of the benefits of a decision tree is that the resulting structure can be output into a human-readable format.

### Divide and conquer

  - Decision trees are built using a heuristic called <b>recursive partitioning</b>, also known as <b>divide and conquer</b>.
  - It splits the dat into subsets, which are then split repeatedly into even small subsets, and so on.  This process stops when the algorithm determines the data within the subsets are sufficiently homogeneous, or another stopping criterion has been met.

### The C5.0 decision tree algorithm

  - <b>C5.0 algorithm</b> is an improved version of <b>C4.5 algorithm</b>, which itself is an improvement over <b>Iterative Dichotomiser 3 (ID3)</b> algorithm.
  - C5.0 algorithm is the industry standard for production decision trees, because it does well for most types of problems directly out of the box.
    - Strengths
      - All-purpose classifier
      - Highly automatic learning process
      - Excludes unimportant features
      - Can be used on small and large datasets
      - results in a model that can be interpreted
      - More efficient than other complex models
    - Weaknesses
      - Decision tree models are often biased towards splits on features haveing a large number of levels
      - It is easy to overfit or underfit
      - Can have trouble modeling some relationships do to reliance on axis-parallel splits
      - Small changes in training data can result in large changes
      - Large trees can be difficult to interpret and the decisions they make may seem counterintuitive

### Choosing the best split

  - First challenge in a decision tree is identifying which features to split on.
  - Purity : the degree to which a subset of examples contain only a single class
  - Pure : anysubset compose of only a single class
  - <b>C5.0</b> uses entropy, a concept borrowed from information theory that quantifies the randomness, or disorder, within a set of class values.
    - Sets with high entropy are very diverse.
    - Entropy is meansured in <b>bits</b>.  For n classes, entropy ranges from - to log2(n).
      - Entropy is specified as Entropy (S) = sum(-p[i] * log2(p[i]))
```{r}
# - Example, we have a partition of data with two classes : red (60 percent) and white (40 percent)
-0.60 * log2(0.60) - 0.40 * log2(0.40)

# - We can visualize the entropy for all possible two-class arrangements:
curve(-x * log2(x) - (1 - x) * log2(1 - x), 
      col = "red", xla = "x", ylab = "Entropy", lwd = 4)
```  

  - To use entropy to determine the optimal feature to split upon, the algorithm calculates the changes in homogeneity that would result from a split on each possible feature, a measure known as <b>information gain</b>.
    - InfoGain(F) = Entropy(S1) - Entropy(S2)
  - The function to calculate Entropy needs to consider the total entropy across all the partitions.  It does this by weighting each partition's entropy according to the proportion of all records falling into partition.
    - Entropy(S) = sum(w[i] * Entropy(P[i]))
  - The higher the information gain, the better a feature is at creating homogeneous groups after a split on that feature.  If information gain is zero, there is no reduction in entropy for splitting on on this feature.

### Pruning the decision tree

  - If the tree groups overly large, many of the decision it makes will be overly specific and the model will be overfitted to the training data.
  - <b>Pruning</b> is the process of reducing it's size such that it generalizes better to unseen data.
  - One solution is to stop the tree from growing once it reaches a certain number of decisions.  This is called <b>early stopping</b> or <b>pre-pruning</b> the decision tree.
  - <b>Post-pruning</b> involves growing a tree that is intentionally too large and pruning leaf nodes to reduce the size of the tree to a more appropriate level.  This is often more effective.
  - The C5.0 algorithm is opinionated about pruning - it takes care of many of the decisions automatically.
  - The process of grafting branches is known as <b>sub-tree raising</b> and <sub-tree replacement</b>.

## Example - identifying risky bank loans using C5.0 decision trees

### Step 1 - collecting data

  - Data for this example is the dataset donated to the UCI Machine Learning Repository http://archive.ics.uci.edu/ml

### Step 2 - exploring and preparing the data

```{r}
# - Load the data, because the character data is entirely categorical, we can omit stringsAsFactors
credit <- read.csv("credit.csv")

# - There's an error in the code where the C50 is reading the default as numbers and not as.factors convert the column.
credit$default<-as.factor(credit$default)

# - check our resulting object
str(credit)

# - Use table() to check the output for a couple of loan features
table(credit$checking_balance)
table(credit$savings_balance)

# - Some of the loan's features are numeric
summary(credit$months_loan_duration)
summary(credit$amount)

# - The default vector indicates whether the loan was able to meet the agreed payment terms or if they went into default.
table(credit$default)
```

### Data preparation - creating random training and test datasets

```{r}
# - First step is that we need to split our data set into training and test sets.
# - Since the dataset we have isn't in a random order then we need to use a <b>random sample</b> to try and get a representative sample.
# - The following commands use sample() with seed value.  
#RNGversion("3.5.2"); 
set.seed(123)
train_sample <- sample(1000, 900)
str(train_sample)

# - By using this vector to select rows from our data
credit_train <- credit[train_sample, ]
credit_test <- credit[-train_sample, ]

# - If randomization was done correctly, we should have about 30 percent of loans
prop.table(table(credit_train$default))
prop.table(table(credit_train$default))
```


### Step 3 - training a model on the data

```{r}
# - We will use the C5.0 algorithm in the c50 package for training our decision tree model.
#install.packages("C50")
library(C50)

# - C5.0 decision tree syntax
#   - Building the classifier:
#     - $ m <- c5.0(train, class, trails = 1, costs = NULL)
#     - train is a data frame containing training data
#     - class is a factor vector with the class for each row in the training data
#     - trials is an optional numbner to control the number of boosing iterations
#     - costs is an optional matrix specifying costs associated with various types
#   - Making predictions
#     - $ p <- predict(m, test, type="class")
#     - m is the model trained by the c5.0() function
#     - test is the data frame containing test data with the same features as the training data
#     - type is either "class" or "prob" ans specifies whether the predictions should be the most probable class value or raw predicted probabilities

# - For the first iteration, we'll use default settings
credit_model <- C5.0(credit_train[-17], credit_train$default)
#credit_model <- C5.0(default ~ ., data = credit_train)

# - the output shows some simple facts about the tree including the function call that generated it, the number of features (labeled predictors), and examples (labeled samples) used to grow the tree.
credit_model

# - To see the tree's decisions, we can call the summary function
summary(credit_model)
```


### Step 4 - evaluating model performance

```{r}
# - Apply our decision tree
credit_pred <- predict(credit_model, credit_test)

# create a CrossTable, to compare our predictions to the truth values
library(gmodels)
CrossTable(credit_test$default, credit_pred,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual default', 'predicted default'))
```

## Step 5 - improving model performance

### Boosting the accuracy of decision trees

```{r}
# - Adaptive boosting : this is a process in which many decision trees are built and trees vote on the best class for each example
# - C5.0() makes it easy to add boosting to our decision tree.  We simply add trials parameter, indicating the number of seperate decision trees to use in a boosted team.
credit_boost10 <- C5.0(credit_train[-17], credit_train$default, trials = 10)
credit_boost10

summary(credit_boost10)


credit_boost_pred10 <- predict(credit_boost10, credit_test)
CrossTable(credit_test$default, credit_boost_pred10,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual default', 'predicted default'))
```

### Making some mistakes cost more than others

```{r}
# - The C5.0 algorithm allows us to assign a penalty to different types of errors in order to discourage a tree from making more costly mistakes.
# - The penalties are designated a cost matrix
matrix_dimensions <- list(c("no", "yes"), c("no", "yes"))
names(matrix_dimensions) <- c("predicted", "actual")

# - Examining the new object to make sure our dimensions have been set up correctly:
matrix_dimensions

# - Next assign penalties for the various types of errors by supplying for values to fill the matrix
error_cost <- matrix(c(0, 1, 4, 0), nrow=2, 
                     dimnames = matrix_dimensions)
error_cost

# - To see how this impacts classifications, let's apply it to our decision tree
credit_cost <- C5.0(credit_train[-17], credit_train$default,
                    costs = error_cost)
credit_cost_pred <- predict(credit_cost, credit_test)
CrossTable(credit_test$default, credit_cost_pred,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual default', 'predicted default'))
```

## Understanding classification rules
  - Classification rules represent knowledge in the form of logical if-else statements that assign a class to unlabeled examples.
  - They are specified in terms of an <b>antecedent</b> and a <b>consequent</b>, which forms a statement that says "if this happens, then that happens"
  - Rule learners are closely related sibling of decision tree learners and are oftern used for similar types of tasks. They are used for:
    - identifying conditions that lead to hardware failures in mechanical devices.
    - describing the key characteristics of groups of people for customer segmentation
    - finding conditions that precede large drops or increases in the prices of shares of the stock market

### Separate and conquer
  - Classification rule learning algorithms utilize a heuristic known as <b>seperate and conquer</b>.
  - As rules seem to cover portions of the data, separate and conquer algorithms are also known as <b>covering algorithms</b>, and the resulting rules are called covering rules.

## The 1R algorithm
  - <b>ZeroR</b> : simplest classifier which is a rule learner that considers no features and literally learns no rules.  For every unlabeled example, regardless of the values of its features it predicts the most common class.
  - <b>1R algorithm (One Rule or OneR)</b>, improves over ZeroR by selecting a single rule.
    - Strengths
      - Generates a single, easy-to-understand human-readable rule
      - Often performs surprising well
      - Can serve as benchmark for more complex algorithms
    - Weaknesses
      - Uses only a single feature
      - probably overly simplistic

## The RIPPER algorithm
  - <b>Incremental reduced error pruning (IREP) algorith</b> : uses a combination of pre-pruning and post-pruning methods that grow very comples rules and prune them before separating the instances from the full dataset.
  - <b>Repeated incremental pruning to produce error reduction (RIPPER) algorithm</b> : improved upon IREP to generate rules that match or exceed the performance of decision trees.
    - Strengths
      - Generates easy-to-understand, human-readable rules
      - Efficent on large and noisy datasets
      - Generally, produces a simpler model than a comparable decision tree
    - Weaknesses
      - May result in rules that seem to defy common sense or expert knowledge
      - Not ideal for working with numeric data
      Might not perform as well as more complex models
  - Has a three step process : grow, prune, optimize
    - Grow : usesw seperate and conquer technique to greedily add conditions to a rule until it perfectly classifies
    - When increasing a rule's specificity no longer reduces entropy, the rule is immediately pruned
    - Grow and Prune are repeatedly performed until reaching a stopping criterion, at which point the entire set of rules is optimized.

### Rules from decision trees
  - Classification rules can also be obtained from a decision tree.

### What makes trees and rules greedy?
  - Decision trees and rule learners are known as <b>greedy learners</b> because they use data on a first-come, first-served basis.
  - Both attempt to make partitions one at a time, finding the most homogeneous partion first.

## Example - identifying poisonous mushrooms with rule learners
### Step 1 - collection the data
  - To identify rules for distinguishing poisonous mushrooms, we will utilize mushrooms data set.  Http://archive.ics.uci.edu/ml
  - Download from https://github.com/PacktPublishing/Machine-Learning-with-R-Fourth-Edition/blob/main/Chapter%2005/mushrooms.csv

### Step 2 - exploring and preparing the data
```{r}
# - Read in our dataset
mushrooms <- read.csv("mushrooms.csv", stringsAsFactors = TRUE)
str(mushrooms)

# - Since the veil_type doesn't vary, go ahead and drop
mushrooms$veil_type <- NULL

# - Look at the distribution of the mushroom type
table(mushrooms$type)

```

### Step 3 - training a model on data
```{r}
# - Since simpl;e rules can still be useful, let's see how a very simple rule learner performs on the mushroom data.
# - 1R implementation found in the OneR backage.
#install.packages("OneR")
library(OneR)

# - 1R classification rule syntax
#   - Building the classifier:
#     - m <- OneR(class ~ predictors, data = mydata)
#     - class is the column in the mydata data frame to be predicted
#     - predictors is an R formula specifying the features in mydata data frame
#     - data is the data frame in which class and predictors can be found
#   - Making predictions
#     - p <- predict(m, test)
#     - m is a model trained by the OneR() function
#     - test is a data frame containing test data with the same features as the training data used to build the classifier

# - OneR() function uses the R formula syntax to specify the model to be trained.  
#   - The formula syntax uses the ~ operator to express the relationship between a target variable and its predictors.
#   - The class variable to be learned goes to the left of the tild
#   - The predictor features are written on the right, seperated by + operators
#   - If you would like to model the relationship between the class y and the predictors x1 and x2, the formula would be y ~ x1 + x2.
#   - The '.' allows our rule learner to consider all posible features
mushroom_1R <- OneR(type ~ ., data = mushrooms)
mushroom_1R
```

### Step 4 - evaluating model performance
```{r}
# - Examine the confusion matrix of the predicted versus actual values.
mushroom_1R_pred <- predict(mushroom_1R, mushrooms)
table(actual = mushrooms$type, predicted = mushroom_1R_pred)
```

### Step 5 - improving model and performance
```{r}
# - We will use the JRip(), a java based implementation of the RIPPER algorithm.
#install.packages("RWeka")
library(RWeka)

# - RIPPER classification rule syntax
#   - Building the classifier
#     - m <- JRip(class ~ predictors, data = mydata)
#     - class is the column in the mydata data frame to be predicted
#     - predictors is an R formula specifying the features in mydata data frame to use predicton
#     - data is the data frame in which class and predictors can be found
#   - Making predictions
#     - p <- predict(m, test)
#     - m is a model trained by JRip() function
#     - test is a data frame containing tesst data with the same features as the training data used to build the classifier

mushroom_JRip <- JRip(type ~ ., data = mushrooms)
mushroom_JRip

mushroom_JRip_pred <- predict(mushroom_JRip, mushrooms)
table(actual = mushrooms$type, predicted = mushroom_JRip_pred)
```
