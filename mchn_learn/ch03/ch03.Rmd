---
Machine Learning with R
Chapter 03 - lazy Learning - classification Using Nearest Neighbors
---

### Understanding nearest neighbor classification

- nearest neighbor classifiers are defined by their characteristic of classifying unlabled examples by assigning them the class of similar labeled examples.
- examples
  - Computer vision applications
  - Recommendation systems that predict whether a person will enjoy a movie or song
  - Identifying patterns in genetic data to detect specific proteins / diseases.
  

### The k-NN algorithm

- k-nearest-neighbors algorithm (k-NN), is a classification example
- Strengths
  - Simple and effective
  - Makes no assumptions
  - Fast training phase
- Weaknesses
  - does not produce a model, limiting the ability to understand how the features are related to the class
  - requires selection of an appropriate k
  slow classification phase
  - Nominal features and missing data require additional processing

### Measuring similarity with distance 

  - Locating the nearest neighbor requires a distance function
  - Example functions
    - Euclidean distance
      - dist(p,q) = sqrt((p[1] - q[1])^2 + (p[2] - q[2])^2 + ... + (p[n] - q[n])^2)
    - Manhattan distance
    

### Choosing an appropriate k
  - The balance between over-fitting and under-fitting the training data is a problem know as bias-variance tradeoff.
  - Choose a large k reduces the impact or variance caused by noisy data, but can bias the learner such that it runs the risk of ignoring small, but important patterns.

### Preparing data for use with k-NN

  - The traditional method of re-scaling features for k-NN is min-max normalization.
    - x[new] = (x - min(x))/(max(x) - min(x))
  - Z-score standardization
    - z-scores fall into an unbounded range of negative and positive number.
    - x[new] = (X - mean(X))/stddev(X)
  - Dummy coding: process of converting a categorical variable into a numeric values
  - One-hot encoding: where each category is converted into it's own columns that is 1 or 0

### Why is the k-NN algorithm lazy?

  - No abstraction is involved in the k-NN,
  - Under the strict definition of learning, a lazy learner is not really learning anything.  Instead, it merely stores the training data verbatim.

## Example - diagnosing breast canver with k-NN

  - If machine learning could automate the identification of cancerous cells, it would provide considerable benefits of the health system.

### Step 1 - collection data

  - The data set utilized is the Breast Cancer Wisconsin (Diagnostic) dataset from the UCI Machine Learning Repository at http://archive.ics.uci.edu/ml
  - The data include 569 examples of cancer biopsies, each with 32 features.
  The 30 numeric measurements comprise the mean, standard error and worst value (largest) for 10 different characteristics of the digitized cell nuclei.
    - Radius, Texture, Perimeter, Area, Smoothness, Compactness, Concavity, Concave points, Symmetry, Fractal dimension, 


### Step 2 - exploring and preparing the data

```{r}
# - Begin by importing the CSV data file
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)

# - Use the str command to confirm that the data is structured
str(wbcd)

# - Let's drop the id feature altogether.
wbcd <- wbcd[-1]

# - The variable diagnosis, is the outcome we want to predict
table(wbcd$diagnosis)

# - MOur R machine learning classifiers require the target feature to be coded as a factor.  recode the diagnosis variables
wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B", "M"), labels = c("Benign", "Malignant"))

# - When we look at the prop.table() output, we now find that values have been Benign and Malignant.
round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)

# - The remaining 30 features are all numeric and, as expected, consist of three different measurements of 10 characteristics.
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
```


### Transformation - normalizaing numeric data

```{r}
# - 
```


### Vectors

```{r}
# - 
```


### Vectors

```{r}
# - 
```


### Vectors

```{r}
# - 
```


### Vectors

```{r}
# - 
```


### Vectors

```{r}
# - 
```


### Vectors

```{r}
# - 
```
