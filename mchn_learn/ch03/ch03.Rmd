---
Machine Learning with R
Chapter 03 - lazy Learning - classification Using Nearest Neighbors
---

### Understanding nearest neighbor classification

- nearest neighbor classifiers are defined by their characteristic of classifying unlabled examples by assigning them the class of similar labeled examples.
- examples
  - Computer vision applications
  - Recommendation systems that predict whether a person will enjoy a movie or song
  - Identifying patterns in genetic data to detect specific proteins / diseases.
  

### The k-NN algorithm

- k-nearest-neighbors algorithm (k-NN), is a classification example
- Strengths
  - Simple and effective
  - Makes no assumptions
  - Fast training phase
- Weaknesses
  - does not produce a model, limiting the ability to understand how the features are related to the class
  - requires selection of an appropriate k
  slow classification phase
  - Nominal features and missing data require additional processing

### Measuring similarity with distance 

  - Locating the nearest neighbor requires a distance function
  - Example functions
    - Euclidean distance
      - dist(p,q) = sqrt((p[1] - q[1])^2 + (p[2] - q[2])^2 + ... + (p[n] - q[n])^2)
    - Manhattan distance
    

### Choosing an appropriate k
  - The balance between over-fitting and under-fitting the training data is a problem know as bias-variance tradeoff.
  - Choose a large k reduces the impact or variance caused by noisy data, but can bias the learner such that it runs the risk of ignoring small, but important patterns.

### Preparing data for use with k-NN

  - The traditional method of re-scaling features for k-NN is min-max normalization.
    - x[new] = (x - min(x))/(max(x) - min(x))
  - Z-score standardization
    - z-scores fall into an unbounded range of negative and positive number.
    - x[new] = (X - mean(X))/stddev(X)
  - Dummy coding: process of converting a categorical variable into a numeric values
  - One-hot encoding: where each category is converted into it's own columns that is 1 or 0

### Why is the k-NN algorithm lazy?

  - No abstraction is involved in the k-NN,
  - Under the strict definition of learning, a lazy learner is not really learning anything.  Instead, it merely stores the training data verbatim.

## Example - diagnosing breast canver with k-NN

  - If machine learning could automate the identification of cancerous cells, it would provide considerable benefits of the health system.

### Step 1 - collection data

  - The data set utilized is the Breast Cancer Wisconsin (Diagnostic) dataset from the UCI Machine Learning Repository at http://archive.ics.uci.edu/ml
  - The data include 569 examples of cancer biopsies, each with 32 features.
  The 30 numeric measurements comprise the mean, standard error and worst value (largest) for 10 different characteristics of the digitized cell nuclei.
    - Radius, Texture, Perimeter, Area, Smoothness, Compactness, Concavity, Concave points, Symmetry, Fractal dimension, 


### Step 2 - exploring and preparing the data

```{r}
# - Begin by importing the CSV data file
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)

# - Use the str command to confirm that the data is structured
str(wbcd)

# - Let's drop the id feature altogether.
wbcd <- wbcd[-1]

# - The variable diagnosis, is the outcome we want to predict
table(wbcd$diagnosis)

# - MOur R machine learning classifiers require the target feature to be coded as a factor.  recode the diagnosis variables
wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B", "M"), labels = c("Benign", "Malignant"))

# - When we look at the prop.table() output, we now find that values have been Benign and Malignant.
round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)

# - The remaining 30 features are all numeric and, as expected, consist of three different measurements of 10 characteristics.
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
```


### Transformation - normalizaing numeric data

```{r}
# - To normalizae these features, we need to create a normalize() function.
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

normalize(c(1, 2, 3, 4, 5))
normalize(c(10, 20, 30, 40, 50))

# - The lapply() function takes a list and applies a specified function to each list elemnet.
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))

# - to confirm the transformation was applied correctly.
summary(wbcd_n$area_mean)
```


### Data preparation - creating training and test datasets

```{r}
# - We must divide our data into two portions: a training data set that will be used to build the k-NN model and a test dataset that will be used to estimate the predictive accuracy of the model

wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]

# - We need to do the same with the with the labels that weren't included in the original datasets.

wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
```


### Step 3 - training a model on the data

```{r}
# - To classify our test instance we will use a k-NN implementation from the class package
install.packages("class")
library(class)

# - Training and classification using the knn() function is performed in a single command with four parameters
# - No we use the knn() function to classify the test data:
wbcd_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 3)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 21)
```


### Step 4 - evaluating model performance

```{r}
install.packages("gmodels")
library(gmodels)

# - we need to evaluate how well the predicted classes in wbcd_test_pred vector match the actual values in wbcd_test_lables.
# - Use the CrossTable function in the gmodels package
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq = FALSE)

# - True negative - TOP LEFT (correctly identified as benign)
# - True positive - BOTTOM RIGHT (correctly identified as malignant)
# - False negative - BOTTOM LEFT (in-correctly identified as benign)
# - False positive - TOP RIGHT (in-correctly identified as malignant)

```


### Step 5 - improving model performance

#### Transformation - z-score standardization

```{r}
# - Although normalization is commonly used for k-NN classification, z-score standardization may be more appropriate way to rescale features.
# - Z-score standardization have no predefined minimum and maximum, extreme values are not compressed twoards the center.
wbcd_z <- as.data.frame(scale(wbcd[-1]))

# - Confirm that the transformation was applied correctly
summary(wbcd_z$area_mean)

# - The mean of z-score standardized variable should always be zero
# - As before break up our data into train / test
wbcd_train <- wbcd_z[1:469, ]
wbcd_test <- wbcd_z[470:569, ]
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 21)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq = FALSE)
```


### Testing alternative values of k

  - We may be able to optimize the performance of the k-NN be eximing it's performance across various k values.  
