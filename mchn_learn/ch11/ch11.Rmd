---
Machine Learning with R
Chapter 11 - Improving Model Performance
---

  - In this chapter you will learn:
    - How to automate model performance tuning by ssystematically searching for optimal set of training conditions
    - Methods for combining models into groups that use teamwork to tackle tough learning tasks
    - How to apply a variant of decision trees that has quickly become popular due to its impressive performance

## Tuning stock models for better performance

  - <b>parameter tuning</b> : adjusting the model options to identify the best fit.

### Using caret for automated parameter tuning

  - Automated parameter tuning requires you consider three questions:
    - What type of machine learning model (and specific implementation) should be trained on the data?
    - Which model parameters can be adjusted, and how extensively should they be tuned to find the optimal settings?
    - What criteria should be used to evaluate the models to find the best candidate?
  - For a complete list of models and corresponding tuning parameters covered by caret refer to http://topepo.github.io/caret/modelList.html
  - modelLookup() function can be used to lookup parameters for tuning

```{r}
#install.packages("caret")
library(caret)

modelLookup("C5.0")
```

  - By default, caret searches at most three values for each of the models p parameters.
  - The third and final step in automatic model tuning involves identifying the best model among the candidates.

### Creating a simple tuned model

  - We'll use the C5.0 decision trees with the credit model to optimize this model.

```{r}
credit <- read.csv("../ch05/credit.csv")
credit$default<-as.factor(credit$default)

library(caret)
RNGversion("3.5.2")
set.seed(300)
m <- train(default ~ ., data = credit, method = "C5.0")
m

#   - Four main components in the output:
#     - The first section is a brief description of the input data-set.
#     - The second section is a report of the pre-processing and re-sampling methods 
#       applied
#     - The third section is a list of the candidate models evaluated
#     - The fourth section is a choice of the best model.
#   - The train() function uses its tuning parameters to build a model on the full 
#     data-set, which is stored in the m list object as m$finalModel.
p <- predict(m, credit)

# The resulting vector of predictions works as expected, create a confusion matrix:
table(p, credit$default)

# - All data preparation steps applied by the train() function will be similarly 
#   applied to the data used for predictions.
# - The prediction function provides a standardized interface for obtaining predicted
#   class values
head(predict(m, credit))

# - To obtain the estimated probabilities for each class, use the type = "prob".
head(predict(m, credit, type = "prob"))
```

### Customizing the tuning process

  - You can modify the model selection process.
  - The trainControl() function is used to create a set of configuration options known as a <b>control object</b>.
  - For the trainControl() function, the method parameter is used to set the resampling method, such as holdout sampling or k-fold CV.

<table>
  <tr><th>Re-sampling Method</th><th>Method Name</th><th>Additional Options and Default Values</th></tr>
  <tr><td>Holdout sampling</td><td>LOGOCV</td><td>p = 0.75 (training data proportion)</td></tr>
  <tr><td>k-fold CV</td><td>CV</td><td>number = 10 (number of folds)</td></tr>
  <tr><td>Repeated k-fold CV</td><td>repeatedcv</td><td>number = 10 (number of folds)<br />repeats = 10 (number of iterations)</td></tr>
  <tr><td>Bootstrap sampling</td><td>boot</td><td>number = 25 (resampling iterations)</td></tr>
  <tr><td>0.632 bootstrap</td><td>boot632</td><td>number = 25 (resampling iterations)</td></tr>
  <tr><td>Leave-one-out CV</td><td>LOOCV</td><td>None</td></tr>
</table>

  - The selectionFunction parameter is used to specify the function that will choose the optimal model among the various candidates.

```{r}
# - To create a control object named ctrl that uses 10-fold CV and the oneSE 
#   selection function, use the following command (note that number = 10 is 
#   included only for clarity; since this is the default value for method = "cv")

ctrl <- trainControl(method = "cv", number = 10,
                     selectionFunction = "oneSE")

# - The next step in defining our experiment is to create the grid of parameters
#   o optimize.
# - We can use the expand.grid() function, which creates data frames from the 
#   combinations of all values supplied.
grid <- expand.grid(model = "true",
                    trials = c(1, 5, 10, 15, 20, 25, 30, 35),
                    winnow = FALSE)
grid()

# - The train() function will build a candidate model for evaluation using each 
#   row's combination of model parameters.
# - Given this search grid and control list created previously
RNGversion("3.5.2")
set.seed(300)
m <- train(default ~ ., data = credit, method = "C5.0",
           metric = "Kappa", trControl = ctrl, 
           tuneGrid = grid)

# - This results in an object that we can view by typing its name:
m
```

## Impriving model performance with meta-learning

  - It is possible to combine several models to form a powerful team.
  - The technique of combining and managing the predictions of multiple models fills into a wider set of <b>meta-learning</b> methods.

### Understanding ensembles 

  - The meta-learning approach that utilizes a similar principle of creating a varied team of experts is known as an <b>ensemble</b>.
  - The <b>allocation function</b> dictates how much of the training data each model receives.
    - The allocation function can increase diversity by artificially varying the input data to bias the resulting learners, even if they are the same type.
  - The <b>combination function</b> : governs how disagreements among the predictions are reconciled.
    - After the ensemble's models are constructed, they can be used to generate a set of predictions, which must be managed in some way.
    - For example, the ensemble might use a majority vote to determine the final prediction, or it could use a more complex strategy such as weighting each model's votes based on its prior performance.
    - The process of using the predictions of several models to train a final arbiter model is known as <b>stacking</b>.
  - One of the benefits of using ensembles is that they allow you to spend less time in pursuit of a single best model.  Number of performance advantages over single models
    - <b>Better generalizability of future problems</b>: the opinions of several learners are incorporated into a single final prediction, no single bias is able to dominate.
    - <b>Improved performance on massive or minuscule data-sets</b>: ensembles also do well on the smallest datasets because resampling methods like bootstrapping are inherently part of many ensemble designs.
    - <b>The ability to synthesize data from distinct domains</b>: there is no one-size-fits-all learning algorithm, the ensemble's ability to incorporate evidence from multiple types of learners is increasingly important
    - <b>A more nuanced understanding of difficult learning tasks</b>: Real-world phenomena are often extremely complex, with many interacting intricacies.

### Bagging

  - One of the first ensemble methods to to game acceptance use a technique called <b>bootstrap aggregating</b> or <b>bagging</b>.
  - Bagging generates a number of training data-sets by bootstrap sampling the original training data.  These data-sets are then used to generate a set of models using a single learning algorithm.  The model's predictions are combined using voting or averaging.
  - It can perform quite well as long as it used with relatively <b>unstable</b> learners, those generating models that tend to change substantially when the input data changes only slightly.
  - The ipred package offers a classic implementation of bagged decision trees.

```{r}
#install.packages("ipred")
library(ipred)
RNGversion("3.5.2")
set.seed(300)
mybag <- bagging(default ~ ., data = credit, nbagg = 25)

# - The model works as expected with the predict() function:
credit_pred <- predict(mybag, credit)
```

## 

  - 

## 

  - 

## 

  - 

## 

  - 

### 

```{r}

```

## 

  - 

