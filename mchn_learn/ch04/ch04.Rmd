---
Machine Learning with R
Chapter 04 - Probabilistic Learning - Classification Using Naive Bays
---

  - Naive Bays algorithm, uses probabilities:

### Understanding Naice Bayes
  - Bayesian methods
  - Classifiers based on Bayesian methods utilize training data to calculate the probability of each outcome based on the evidence provided by feature values.
  - Bayesian classifiers have been used for:
    - text classification, intrustion or anomaly detection, or diagnosing medical conditions

#### Basic concepts of Bayesian methods
  - Bayesian probability theory is rooted in the idea that the estimated likelihood of an <b>event</b>, or potential outcome, should be based on  the evidence at hand across multiple <b>trials</b>, or opportunities for the event to occur.

### Understanding probability
  - The probability of an event is estimated from observed data by dividing the number of trials n which the event occurred by the total number of trials.
  - The probability of all possible outcomes of a trial must always sum to one.
  - Mutually exclusive and exhaustive events : implies that events cannot occur at the same time and are the only possible outcomes.
  - Complement - an event comprising the outcomes in which the event of interest does not happen.

### Understanding joint probability
  - Certain events occur concurrently.  For instance, the probability of an email being spam that contains the word "Viagra".
  - Venn diagram, diagram of circles to illustrate the overlap between sets of items.
  - Intersection: the probability of two events occurring, for instance is an email spam (P(S)) and also contains the word "Viagra" (P(V)).
  - Joint probability : the probability of one event is related to the probability of the other.
  - Independent events : two events are totally unrelated
  - Dependent events : probability of one event is based on the other event.

### Computing conditional probability with Bayes' theorem
  - Bayes' theorem - provides a way of thinking about how to revise an estimate of the probability of one event in light of the evidence provided by another:
    - P(A|B) = P(A intersect B)/P(B)
    - P(A|B) = P(A intersect B)/P(B) = P(B|A)*P(A)/P(B)
  - Conditional probability : probability of A is dependent (or conditional) on what happened with event B.
  - Prior probability - probability without knowledge of next event
  - Probability is also called likelihood
  - Marginal likelihood - the probability of B occurring, without regard to A
  - Frequency table - records the number of times events have occurred.
  - Likelihood table - records the likelihood of an event occurring given A.

### The Naive Bayes algorithm
  - The <b>Naive Bayes</b> algorithm defines a simple method to apply Bayes' theorem to classification. problems.
  - The Naive Bayes algorithm is names as such because it makes some "naive" assumption about the data.
    - It assumes all of the features in the dataset are <b>equally important and independent</b>

### Classification with Naive Bayes
  - <b>Class-conditional Independence</b> : events are independent so long as they are conditioned on the same class value.
  - The conditional independence assumption allows us to use the probability rule for independent events, which states that P(A intersect B) = P(A) * P(B).

### The Laplace estimator
  - If an event never occurs for one or more levels of the class and therefore their joint probability is zero.
  - Because probabilities in Naive Bayes formula are multiplied in a chain, this zero percent value causes the posterior probability of spam to be zero, effectively nullifies and overrules all of the other evidence.
  - Solution is the Laplace estimator.  The Laplace estimator adds a small number to each of the counts in the frequency table, which ensures that each feature has a non-zero probability of occurring with each class.

### Using numeric features with Naive Bayes
  - Naive Bayes uses frequency tables for learning the data, which means that each feature must be categorical.
  - To allow for using numeric data, on solution is to <b>discretize</b> numeric features, which simply means that numbers are put into categories known as <b>bins</b>.  Sometimes called <b>binning</b>.
  - One easiest way is to explore the data for natural categories or <b>cut points</b> in the distribution.
  
## Example - filtering mobile phone spam with the Naive Bayes algorithm

### Step 1 - collecting data
  - To develop the Naive Bayes classifier, use the SMS Spam Collection at http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/ (doesn't seem to exist).

### Step 2 - exploring and preparing the data

```{r}
# - We will transform our data into a representation known as bag-of-words, which ignores word order and simply provides a variable indicating whether the word appears at all.

# We'll begin by import csv data
sms_raw <- read.csv("sms_spam.csv", stringsAsFactors = FALSE)
str(sms_raw)

# - Convert type to categorical
sms_raw$type <- factor(sms_raw$type)

str(sms_raw$type)
table(sms_raw$type)
```

#### Data preparation - cleaning and standardizing text data

```{r}
# - SMS message are strings of text composed of words, spaces, numbers, and punctuation.
# - One needs to remove number and punctuation; hand uninteresting words such as and, but, and or; and how to break apart sentences into individual words.
# - We are going to use the tm package to handle our NLP tasks
#install.packages("tm")
library(tm)

# - first step in processing text data involves creating a corpus
#- Use VCorpus function, which refers to a volatile corpus (volatile refers to that fact that is is only stored in active memory).
# - We'll use VectorSource() reader function to create a source object from the existing sms_raw$text
sms_corpus <- VCorpus(VectorSource(sms_raw$text))

# - By printing the corpus, we see that it contains documents for each of the 5559 SMS messages
print(sms_corpus)

# - we can use list operations to select documents
inspect(sms_corpus[1:2])

# - Use character() function to view the actual message
as.character(sms_corpus[[1]])

# - Use lapply to apply as.character to multiple items
lapply(sms_corpus[1:2], as.character)

# - to perform our analysis, we need to divide these messages into individual words.
# - First, we will clean the text to standardize the words and remove punctuation characters.  For instance "Hello!", "HELLO", and "hello" should be counted as the same word.
#   - Our first transformation change messages to only use lowercase characters.
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))

# - Check if the command worked
as.character(sms_corpus[[1]])
as.character(sms_corpus_clean[[1]])

# - Second remove the numbers
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)

# - Third remove filler (stop) words like 'to', 'and', 'but', and 'or'
#   - Use the existing stopwords function from the tm package.
#   - Use the removeWords() function in tm to accomplish this
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords())

# - Fourth eliminate punctuation from text
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)

# example of removePunctuation
removePunctuation("hello...world")

# - Stemming is the process of reducing words to their root form.  It takes words like learned, learning, and learns and strips the suffix in order to transform them into base form, learn.
#install.packages("SnowballC")
library(SnowballC)
# - Example of stemming using snwoball
wordStem(c("learn", "learned", "learning", "learns"))
# - Apply wordStem function to an entire corpus of text documents
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)

# - Finally, strip additional black spaces from the text
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)

# - Compare the first three messages after transformation
as.character(sms_corpus[[1]])
as.character(sms_corpus_clean[[1]])

as.character(sms_corpus[[2]])
as.character(sms_corpus_clean[[2]])

as.character(sms_corpus[[3]])
as.character(sms_corpus_clean[[3]])

```

### Data preperation - splitting text documents into words

```{r}
# - The final step is to split the messages into individual terms through a process called tokenization
# - The DocumentTermMatrix function takes a corpus and create a data structure called document-term matrix (DTM), in which rows indicate documents and columns indicate terms (words)
# - Sparse matrix : the vast majority of cells in the matrix are filled with zeros (since many words only show up once or twice)
# - Create a DTM sparse matrix
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)

# - You can also use the DTM to perform all of the data cleaning we already did.
sms_dtm2 <- DocumentTermMatrix(sms_corpus, control = list(
  tolower = TRUE,
  removeNumbers = TRUE,
  stopwords = TRUE,
  removePunctuation = TRUE,
  stemming = TRUE
))

# - check for the differences between the sms_dtm and sms_dtm2
sms_dtm
sms_dtm2

# - The reason for the discrepancies has to do with minor difference in the ordering of preprocessing steps.
```

### Data preparation - creating training and test datasets

```{r}
# - Split the data into training and test datasets.
# - Divide the data into two portions: 75 percent for training and 25 percent for testing.
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]

# save a pair of vectors with the labels for each of rows in the training and testing matrices.
sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels <- sms_raw[4170:5559, ]$type

# - confirm that subsets are representative of the complete set of SMS data
prop.table(table(sms_train_labels))
prop.table(table(sms_test_labels))
```

### Visualizing text data - word clouds

```{r}
# - A word cloud is a way to visually depict the frequency at which words apper in text data.
# - Install the wordcloud package
#install.packages("wordcloud")
library(wordcloud)

# - create a word cloud dirctly from a tm corpus
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)

# - Let's use R's subset() function to take a subset of sms_raw data by the SMS type.
spam <- subset(sms_raw, type == "spam")
ham <- subset(sms_raw, type == "ham")

# - Create word clouds from both 
# - TODO : Erroring out, skipping for now
#wordcloud(spam$text, max.words = 40, scale = c(c, 0.5))
#wordcloud(ham$text, max.words = 40, scale = c(c, 0.5))
```

### Data preparation - creating indicator features for frequent words

```{r}
# - The final step of data preparation process is to transform the sparse matrix into a data structure that can be used to train a Naive Bayes classifier.
# - Most of the words don't appear often enough to be useful, so we'll remove anywords that appear in less than five messages (about 0.1 percent of the records).
# - Use findFreqTerms() function to find frequent words
findFreqTerms(sms_dtm_train, 5)

# - Save the character vector
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)

# - Review the contents of the vector
str(sms_freq_words)

# - Now filter our DTM to include only terms appearing in frequent word vector.
sms_dtm_freq_train <- sms_dtm_train[ , sms_freq_words]
sms_dtm_freq_test <- sms_dtm_test[ , sms_freq_words]

# - Naive Bayes classifier is usually trained on data with categorical features.
# - Define a convert_counts function to convert counts to Yes or No
convert_counts <- function(x) {
  x <- ifelse(x > 0, "Yes", "No")
}

# - Use the apply() function (can be used on rows or columns), it uses MARGIN parameter to specify either rows (1) or columns (2).
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2, convert_counts)
sms_test <- apply(sms_dtm_freq_test, MARGIN = 2, convert_counts)
```

## Step 3 - training a model on the data

```{r}
# - Now that our data is properly formatted, train our model using the e1071 package
#install.packages("e1071")
library(e1071)

# - Naive Bays learner occurs in stages:
#   - Building the classifier:
#     - m <- naiveBayes(train, class, laplace = 0)
#       - train is the data frame or matrix containing training data
#       - class is a factor vector with the class for each row in the training data
#       - laplace is a number to control the Laplace estimator (by default 0)
#   - Making predictions:
#     - p <- predict(m, test, type = "class)
#       - m is the model trained
#       - test is a data frame or matrix containing test data with the same features an the training data used to build the classifier
#       - type is either "class" or "raw" and specifies whether the predictions should be the most likely class value or the raw predicted probabilities
# - The function will return a vector of predicted class values or raw predicted probabilities depending upon the value of the type parameter.
sms_classifier <- naiveBayes(sms_train, sms_train_labels)
```

## Step 4 - evaluating model performance

```{r}
# - To evaluate our class fier, we need to test it's predictions on unseen messages in the test data.
sms_test_pred <- predict(sms_classifier, sms_test)

# - Use the CrossTable() function to compare the predictions
library(gmodels)
CrossTable(sms_test_pred, sms_test_labels, 
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('predicted', 'actual')
           )
```

## Step 5 - improving model performance

```{r}
# - To improve our model, try running again but this time with laplace = 1
sms_classifier2 <- naiveBayes(sms_train, sms_train_labels, laplace = 1)
sms_test_pred2 <- predict(sms_classifier2, sms_test)

CrossTable(sms_test_pred2, sms_test_labels, 
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('predicted', 'actual')
           )
```
