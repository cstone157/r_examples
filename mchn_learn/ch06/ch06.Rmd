---
Machine Learning with R
Chapter 05 - Divide and Conquer - Classification Using Decision Trees and Rules
---

## Forecasting Numeric data - regression Methods
- Basic statistical principles used in regression, a technique that models the size and strength of numeric relationships
- How to prepare data regression analysis, and estimate and interpret a regression model
- A pair of hybrid techniques known as regression trees and model trees, which adapt decision tree classifiers for numeric prediction tasks

### Understanding regression
  - <b>Dependent variable</b> - The value to be predicted, is dependent upon the independent variables
  - <b>Independent variable</b> - the predictors
  - <b>Slope-intercept form</b> - "y = a + b*x"  The letter y indicates the dependent variable and x indicates the independent variable.  The <b>slope</b> term b specifies how much the line rises for each increase in x.  "a" is also know as the <b>intercept</b>.
  - Use cases
    - Examining how populations and individuals vary by their measured characteristics
    - Quantifying the casual relationship between an event and its response
    - Identifying patters that can be used to forecast future behavior
  - Regression methods are also used for <b>statistical hypothesis testing</b>,
  - <b>Simple linear regression</b> - there is only a single independent variable.
  - <b>Multiple linear regression</b> - case of two or more independent variables
  - <b>Logistic regression</b> - model binary categorical outcome.
  - <b>Poisson regression</b> models integer count data
  - <b>Multinomial logistic regression</b> models a categorical outcome and can therefore be used for classification

### Simple linear regression
  - Alpha (intercept), Beta(slope).

### Ordinary least squares estimation
  - In order to determine the optimal estimates of a and b, you can use <b>ordinary least squares (OLS)</b>
    - In OLS, the slope and intercept are cosen such that they minimize the <b>sum of the squared errors (SSE)</b>.
    - The errors, also know as <b>residuals</b>, are the vertical distance between the predicted y value and the actual y value.
    - Solve using a = mean(y) - b*mean(x)
```{r}
launch <- read.csv("challenger.csv")

# - If the shuttle launch data is stored in a data frame, the independent variable x named "temperature", and the dependent variable y is named "distress_ct"
b <- cov(launch$temperature, launch$distress_ct) / var(launch$temperature)
b

#- We can estimate "a" using the computed "b" value and apply the mean() function
a <- mean(launch$distress_ct) - b * mean(launch$temperature)
a
```

### Correlations
  - A <b>correlation</b> between two variables is a number that indicates how closely their relationship follows a straight line.
  - Without additional qualification, correlation typically refers to the <b>Pearson correlation coefficient</b>, a correlation rnages between -1 and +1.
    - p(x, y) = Corr(x, y) = Cov(x,y) / (stddev(x)*stddev(y))
```{r}
# - Using the formula, we can calculate the correlation between the launch temperature and the number of O-ring distress
r <- cov(launch$temperature, launch$distress_ct) / (sd(launch$temperature) * sd(launch$distress_ct))
r

# - Alternatively, we can obtain the same result with cor() correlation function:
cor(launch$temperature, launch$distress_ct)

# - The various rules of thumb used to interpret correlation strength.  
#   - One method assigns a status of "weak" to values between 0.1 and 0.3; 
#   - "moderate" to the range of 0.3 and 0.5, 
#   - and "strong" to values above 0.5
```

### Multiple linear regression
  - Strengths
    - The most common approach for modeling numeric data
    - Can be adapted to model almost any modeling task
    - provides estimates of both the size and strength of the relationships among features and outcome
  - Weaknesses
    - Makes strong assumptions about the data
    - The model's form must be specified by the user in advance
    - Does not handle missing data
    - Only works with numeric features, so categorical data requires additional preparation
    - Requires some knowledge of statistics to understand the model
  - y = a + b[1]*x[1] + b[2]*x[2] + ... + b[i]*x[i] + error
  - The many rows and columns of data can be described in a condensed fomulations using <b>matrix notation</b>.
  - The solution uses a pair of matrix operations: the T indicates the transpose of matrix X, while the negative exponent indicates the matrix inverse
```{r}
# - Using the following code, we can create a basic regression function named reg(), which takes a parameter y and a parameter x
reg <- function(y, x) {
  x <- as.matrix(x)
  x <- cbind(Intercept = 1, x)
  b <- solve(t(x) %*% x) %*% t(x) %*% y
  colnames(b) <- "estimate"
  print(b)
}

# - The as.matrix() function converts the data frame into matrix form
# - cbind() function binds an additional column onto the x matrix; the command Intercept = 1 instructs R to name the new column Intercept and to fill the column with repeating 1 values.
# - solve() takes the inverse of a matrix
# - t() is used to transpose a matrix
# - %*% multiplies two matrices

str(launch)

# - we can confir that our function is working correctly by comparing its result for the simple linear regression model of O-ring
reg(y = launch$distress_ct, x = launch[2])

reg(y = launch$distress_ct, x = launch[2:4])

```

## Example - predicting medical expenses using linear regression
### Step 1 - collecting data
  - The data is created using book of demographic statistics from the US Census Bureau

### Step 2 - exploring and preparing the data
```{r}
# - Read the insurance file in
insurance <- read.csv("insurance.csv", stringsAsFactors = TRUE)
str(insurance)

# - Our models dependent variable is expenses
summary(insurance$expenses)

# - Although linear regression does not strictly require a normally distributed dependent variable, the model often fits better when this is true.
summary(insurance$expenses)
hist(insurance$expenses)

# - Regression require that every feature is numeric.  Look at our three factor-type features.
table(insurance$region)
```

### Exploring relationships among features - the correlation matrix
```{r}
# - A correlation matrix provides a quick overview of the relationship between dependent and independent variables.
cor(insurance[c("age", "bmi", "children", "expenses")])
```

### Visualizing relationships among features - the scatterplot matrix
```{r}
# - Instead of creating a scatterplot for each possible relationship, we can create a scatterplot matrix
pairs(insurance[c("age", "bmi", "children", "expenses")])

# - An enchanced scatterplot matrix can be created with pairs.panels()
#install.packages("psych")
library(psych)
pairs.panels(insurance[c("age", "bmi", "children", "expenses")])

# - The oval-shaped object in each scatterplot is a correlation ellipse.  It provides a visualization of correlation strength.
# - The curve drawn on the scatterplot is called a loess curve.  it indicates the general relationship between the x axis and y axis variables.
```

### Step 3 - training a model on the data
```{r}
# - Multiple regression modeling syntax
#   - Building the model:
#     - m <- lm(dv ~ iv, data = mydata)
#     - dv is the dependent variable in the mydata data frame to be modeled
#     - iv is an R formula specifying the independent variable in the mydata data frame to use in the model
#     - mydata specifies the data frame in which the dv and iv variables can be found
#   - Making predictions:
#     - p <- predict(m, test)
#     - m is a model trained by the lm() function
#     - test is a data frame containing test data with the same features as the training data used to build the model.

# - fits a linear regression model that relates the six independent variables to the total medical expenses.
ins_model <- lm(expenses ~ age + children + bmi + sex + smoker + region,
                data = insurance)

# - because the period(.) character can be used to specify all features, an equivalent to the prior command is:
ins_model <- lm(expenses ~ ., data = insurance)
ins_model
```

### Step 4 - evaluating model performance
```{r}
summary(ins_model)

# - Summary results
#   - Residuals : section provides summary statics for the errors in our predictions
#     - A residual is equal to the true value minus the predicted value, the maximum error.
#   - P-value (denoted Pr(>|t|)) provides an estimate of the probability that the true coefficient is zeor different value of the estimate.
#     - Small p-values suggest that true coefficient is very unlikely to be zero
#     - P-vlues that have stars (***), will correspond to the footnotes that specify the significance level met by estimate.
#       - This level is a threshold, chosed prior to building, which will be used to indicate "real" finding, as opposed to those due to change alone; p-values less than the significance level are considered statistically significant.
#   - Multiple R-squared values (coefficient of determination) provides a measure of how well our model as a whole explains the values of the dependent variable.
#     - Similar to correlation coefficient, the closer the value is to 1.0, the better.
#     - Adjusted R-squared value corrects R-squared by penalizing the models with a large number of independent variables.
```

## Step 5 - improving model performance
### Model specification - adding nonlinear relationships
```{r}
# - Typical regression equations :
#   - y = a + b*x
# - To account for nonlinear relationship, we add higher order term :
#   - y = a + b1*x + b2*x^2

# - To add the nonlinear age to the model, we add new variable
insurance$age2 <- insurance$age^2
```

### Transormation - converting a numberic variable to a binary indicator
```{r}
# - Suppose we have a hunch that the effect of a feature is not cumulative, but rather that it has an effect only after specific threshold has been reached.  For instance, BMI may have zero impoct on medical expenditures for individuals in the normal weight range, but it may be strongly related to higher costs for the obese.
# - We can model this relatioship by creating a binary obesity indicator variable that is 1 if the BMI is at least 30 and 0 if it less than 30.
# - To create the feature, we can use the ifelse() function that, for each element in a vector
insurance$bmi30 <- ifelse(insurance$bmi >= 30, 1, 0)
```

### Model specification - adding interaction effects
 - When two features have a combined effect, this is known as interation.
   - If we suspect two variables interact, we can test this hypothesis by adding their interaction to the model.
   - The "*" operator is shorthand that instructs R to model "expenses ~ bmi30 + smokeryes + bmi:smokeryes".

### Putting it all together - an improved regression model
```{r}
# - The improvements we will be be making:
#   - Added a nonliner term for age
#   - Created an indicator for obesity
#   - Specified an interaction between obesity and smoking
ins_model2 <- lm(expenses ~ age + age2 + children + bmi + 
                   sex + bmi30*smoker + region, data = insurance)

summary(ins_model2)
```

### Making predictions with a regression model
```{r}
# - Apply the model to the original training data using the predict function:
insurance$pred <- predict(ins_model2, insurance)

# - compute the correlation between the predicated and actual actual values
cor(insurance$pred, insurance$expenses)

# - Examine the finding as a scatterplot.
plot(insurance$pred, insurance$expenses)
abline(a = 0, b = 1, col = "red", lwd = 3, lyt =2)

# - To estimate the insurance expenses for a 30 year old, overweight, male non-smoker with two children in the Northeast type:
predict(ins_model2, data.frame(age = 30, age2 = 30^2, children = 2,
                         bmi = 30, sex = "male", bmi30 = 1,
                         smoker = "no", region = "northeast"))
```

### 
  - 
  
### 
```{r}
```

