---
Machine Learning with R
Chapter 05 - Divide and Conquer - Classification Using Decision Trees and Rules
---

## Forecasting Numeric data - regression Methods
- Basic statistical principles used in regression, a technique that models the size and strength of numeric relationships
- How to prepare data regression analysis, and estimate and interpret a regression model
- A pair of hybrid techniques known as regression trees and model trees, which adapt decision tree classifiers for numeric prediction tasks

### Understanding regression
  - <b>Dependent variable</b> - The value to be predicted, is dependent upon the independent variables
  - <b>Independent variable</b> - the predictors
  - <b>Slope-intercept form</b> - "y = a + b*x"  The letter y indicates the dependent variable and x indicates the independent variable.  The <b>slope</b> term b specifies how much the line rises for each increase in x.  "a" is also know as the <b>intercept</b>.
  - Use cases
    - Examining how populations and individuals vary by their measured characteristics
    - Quantifying the casual relationship between an event and its response
    - Identifying patters that can be used to forecast future behavior
  - Regression methods are also used for <b>statistical hypothesis testing</b>,
  - <b>Simple linear regression</b> - there is only a single independent variable.
  - <b>Multiple linear regression</b> - case of two or more independent variables
  - <b>Logistic regression</b> - model binary categorical outcome.
  - <b>Poisson regression</b> models integer count data
  - <b>Multinomial logistic regression</b> models a categorical outcome and can therefore be used for classification

### Simple linear regression
  - Alpha (intercept), Beta(slope).

### Ordinary least squares estimation
  - In order to determine the optimal estimates of a and b, you can use <b>ordinary least squares (OLS)</b>
    - In OLS, the slope and intercept are cosen such that they minimize the <b>sum of the squared errors (SSE)</b>.
    - The errors, also know as <b>residuals</b>, are the vertical distance between the predicted y value and the actual y value.
    - Solve using a = mean(y) - b*mean(x)
```{r}
launch <- read.csv("challenger.csv")

# - If the shuttle launch data is stored in a data frame, the independent variable x named "temperature", and the dependent variable y is named "distress_ct"
b <- cov(launch$temperature, launch$distress_ct) / var(launch$temperature)
b

#- We can estimate "a" using the computed "b" value and apply the mean() function
a <- mean(launch$distress_ct) - b * mean(launch$temperature)
a
```

### Correlations
  - A <b>correlation</b> between two variables is a number that indicates how closely their relationship follows a straight line.
  - Without additional qualification, correlation typically refers to the <b>Pearson correlation coefficient</b>, a correlation rnages between -1 and +1.
    - p(x, y) = Corr(x, y) = Cov(x,y) / (stddev(x)*stddev(y))
```{r}
# - Using the formula, we can calculate the correlation between the launch temperature and the number of O-ring distress
r <- cov(launch$temperature, launch$distress_ct) / (sd(launch$temperature) * sd(launch$distress_ct))
r

# - Alternatively, we can obtain the same result with cor() correlation function:
cor(launch$temperature, launch$distress_ct)

# - The various rules of thumb used to interpret correlation strength.  
#   - One method assigns a status of "weak" to values between 0.1 and 0.3; 
#   - "moderate" to the range of 0.3 and 0.5, 
#   - and "strong" to values above 0.5
```

### Multiple linear regression
  - Strengths
    - The most common approach for modeling numeric data
    - Can be adapted to model almost any modeling task
    - provides estimates of both the size and strength of the relationships among features and outcome
  - Weaknesses
    - Makes strong assumptions about the data
    - The model's form must be specified by the user in advance
    - Does not handle missing data
    - Only works with numeric features, so categorical data requires additional preparation
    - Requires some knowledge of statistics to understand the model
  - y = a + b[1]*x[1] + b[2]*x[2] + ... + b[i]*x[i] + error
  - The many rows and columns of data can be described in a condensed fomulations using <b>matrix notation</b>.
  - The solution uses a pair of matrix operations: the T indicates the transpose of matrix X, while the negative exponent indicates the matrix inverse
```{r}
# - Using the following code, we can create a basic regression function named reg(), which takes a parameter y and a parameter x
reg <- function(y, x) {
  x <- as.matrix(x)
  x <- cbind(Intercept = 1, x)
  b <- solve(t(x) %*% x) %*% t(x) %*% y
  colnames(b) <- "estimate"
  print(b)
}

# - The as.matrix() function converts the data frame into matrix form
# - cbind() function binds an additional column onto the x matrix; the command Intercept = 1 instructs R to name the new column Intercept and to fill the column with repeating 1 values.
# - solve() takes the inverse of a matrix
# - t() is used to transpose a matrix
# - %*% multiplies two matrices

str(launch)

# - we can confir that our function is working correctly by comparing its result for the simple linear regression model of O-ring
reg(y = launch$distress_ct, x = launch[2])

reg(y = launch$distress_ct, x = launch[2:4])

```

## Example - predicting medical expenses using linear regression
### Step 1 - collecting data
  - The data is created using book of demographic statistics from the US Census Bureau

### Step 2 - exploring and preparing the data
```{r}
# - Read the insurance file in
insurance <- read.csv("insurance.csv", stringsAsFactors = TRUE)
str(insurance)

# - Our models dependent variable is expenses
summary(insurance$expenses)

# - Although linear regression does not strictly require a normally distributed dependent variable, the model often fits better when this is true.
summary(insurance$expenses)
hist(insurance$expenses)

# - Regression require that every feature is numeric.  Look at our three factor-type features.
table(insurance$region)
```

### Exploring relationships among features - the correlation matrix
```{r}
# - A correlation matrix provides a quick overview of the relationship between dependent and independent variables.
cor(insurance[c("age", "bmi", "children", "expenses")])
```

### Visualizing relationships among features - the scatterplot matrix
```{r}
# - Instead of creating a scatterplot for each possible relationship, we can create a scatterplot matrix
pairs(insurance[c("age", "bmi", "children", "expenses")])

# - An enchanced scatterplot matrix can be created with pairs.panels()
#install.packages("psych")
library(psych)
pairs.panels(insurance[c("age", "bmi", "children", "expenses")])

# - The oval-shaped object in each scatterplot is a correlation ellipse.  It provides a visualization of correlation strength.
# - The curve drawn on the scatterplot is called a loess curve.  it indicates the general relationship between the x axis and y axis variables.
```

### Step 3 - training a model on the data
```{r}
# - Multiple regression modeling syntax
#   - Building the model:
#     - m <- lm(dv ~ iv, data = mydata)
#     - dv is the dependent variable in the mydata data frame to be modeled
#     - iv is an R formula specifying the independent variable in the mydata data frame to use in the model
#     - mydata specifies the data frame in which the dv and iv variables can be found
#   - Making predictions:
#     - p <- predict(m, test)
#     - m is a model trained by the lm() function
#     - test is a data frame containing test data with the same features as the training data used to build the model.

# - fits a linear regression model that relates the six independent variables to the total medical expenses.
ins_model <- lm(expenses ~ age + children + bmi + sex + smoker + region,
                data = insurance)

# - because the period(.) character can be used to specify all features, an equivalent to the prior command is:
ins_model <- lm(expenses ~ ., data = insurance)
ins_model
```

### Step 4 - evaluating model performance
```{r}
summary(ins_model)

# - Summary results
#   - Residuals : section provides summary statics for the errors in our predictions
#     - A residual is equal to the true value minus the predicted value, the maximum error.
#   - P-value (denoted Pr(>|t|)) provides an estimate of the probability that the true coefficient is zeor different value of the estimate.
#     - Small p-values suggest that true coefficient is very unlikely to be zero
#     - P-vlues that have stars (***), will correspond to the footnotes that specify the significance level met by estimate.
#       - This level is a threshold, chosed prior to building, which will be used to indicate "real" finding, as opposed to those due to change alone; p-values less than the significance level are considered statistically significant.
#   - Multiple R-squared values (coefficient of determination) provides a measure of how well our model as a whole explains the values of the dependent variable.
#     - Similar to correlation coefficient, the closer the value is to 1.0, the better.
#     - Adjusted R-squared value corrects R-squared by penalizing the models with a large number of independent variables.
```

## Step 5 - improving model performance
### Model specification - adding nonlinear relationships
```{r}
# - Typical regression equations :
#   - y = a + b*x
# - To account for nonlinear relationship, we add higher order term :
#   - y = a + b1*x + b2*x^2

# - To add the nonlinear age to the model, we add new variable
insurance$age2 <- insurance$age^2
```

### Transormation - converting a numberic variable to a binary indicator
```{r}
# - Suppose we have a hunch that the effect of a feature is not cumulative, but rather that it has an effect only after specific threshold has been reached.  For instance, BMI may have zero impoct on medical expenditures for individuals in the normal weight range, but it may be strongly related to higher costs for the obese.
# - We can model this relatioship by creating a binary obesity indicator variable that is 1 if the BMI is at least 30 and 0 if it less than 30.
# - To create the feature, we can use the ifelse() function that, for each element in a vector
insurance$bmi30 <- ifelse(insurance$bmi >= 30, 1, 0)
```

### Model specification - adding interaction effects
 - When two features have a combined effect, this is known as interation.
   - If we suspect two variables interact, we can test this hypothesis by adding their interaction to the model.
   - The "*" operator is shorthand that instructs R to model "expenses ~ bmi30 + smokeryes + bmi:smokeryes".

### Putting it all together - an improved regression model
```{r}
# - The improvements we will be be making:
#   - Added a nonliner term for age
#   - Created an indicator for obesity
#   - Specified an interaction between obesity and smoking
ins_model2 <- lm(expenses ~ age + age2 + children + bmi + 
                   sex + bmi30*smoker + region, data = insurance)

summary(ins_model2)
```

### Making predictions with a regression model
```{r}
# - Apply the model to the original training data using the predict function:
insurance$pred <- predict(ins_model2, insurance)

# - compute the correlation between the predicated and actual actual values
cor(insurance$pred, insurance$expenses)

# - Examine the finding as a scatterplot.
plot(insurance$pred, insurance$expenses)
abline(a = 0, b = 1, col = "red", lwd = 3, lyt =2)

# - To estimate the insurance expenses for a 30 year old, overweight, male non-smoker with two children in the Northeast type:
predict(ins_model2, data.frame(age = 30, age2 = 30^2, children = 2,
                         bmi = 30, sex = "male", bmi30 = 1,
                         smoker = "no", region = "northeast"))

# - For a female
predict(ins_model2, data.frame(age = 30, age2 = 30^2, children = 2,
                         bmi = 30, sex = "female", bmi30 = 1,
                         smoker = "no", region = "northeast"))

```

## Understanding regression trees and model trees
  - Trees for numeric prediction fall into 2 categories.
    - <b>regression trees</b> were introducted in 1980s as part of the seminal <b>classification and regression tree (CART)</b> algorithm.
      - Regression trees do not use linear regression methods, they make predictions based on the average value of examples that reach a leaf.
    - <b>model trees</b> - grown in the same as regression tress, but each leaf, a multiple linear regression model is built from examples reaching that node.
  
### Adding regression to trees
  - Strengths
    - Combines the strengths of decision trees with ability to model numeric data
    - does not require the user to specify the model in advance
    - uses automatic feature selection which allows the approach to be used with very large number of features
    - May fit some types of data much better than linear regression
    - Does not require knowledge of statistics to interpret the model
  - Strengths
    - Not as well-known as linear regression
    - Requires a large amount of training data
    - Difficult to determine the overall net effect of individual features on the outcome
    - Large trees can become more difficult to interpret than a regression model
  - Data is partitioned using a divide and conquer strategy according to the feature that will result in the greatest increase in homogeneity (homogeneity is entropy in classification trees)
  - For numeric decision trees, homogeneity is measured by statics like variance, standard deviation, or absolute deviation from mean.
  - One common splitting criterion is called <b>standard deviation reduction (SDR)</b>.
    - SDR = stddev(T) - sum(abs(Ti)/abs(len(T)*stddev(Ti))
```{r}
# - Compute SDR for A and B as follows
tee <- c(1, 1, 1, 2, 2, 3, 4, 5, 5, 6, 6, 7, 7, 7, 7)
at1 <- c(1, 1, 1, 2, 2, 3, 4, 5, 5)
at2 <- c(6, 6, 7, 7, 7, 7)
bt1 <- c(1, 1, 1, 2, 2, 3, 4)
bt2 <- c(5, 5, 6, 6, 7, 7, 7, 7)
sdr_a <- sd(tee) - (length(at1) / length(tee) * sd(at1) + 
                      length(at2) / length(tee) * sd(at2))
sdr_b <- sd(tee) - (length(bt1) / length(tee) * sd(bt1) + 
                      length(bt2) / length(tee) * sd(bt2))

sdr_a
sdr_b

# - Since SDR was reduced more for split on B, the decision would use B first
```


## Example - estimating the quality of wines with regression trees and model trees
### Step 1 - collection data
  - We will use data from UCI Machine Learning Repository (http://archive.ics.uci.edu/ml)
  
### Step 2 - exploring and preparing the data
```{r}
wine <- read.csv("whitewines.csv")

# - wine data includes 11 features
str(wine)

# Check the distribution of our data
hist(wine$quality)

# - Divide the dataset into training and testing sets
wine_train <- wine[1:3750, ]
wine_test <- wine[3751:4898, ]
```

### Step 3 - training a model on the data
```{r}
# - Install the rpart package
#install.packages("rpart")
library(rpart)

# - Regression trees syntax
#   - Building the model :
#     - m <- rpart(dv ~ iv, data = mydata)
#     - dv is the dependent variable in the mydata data frame to be modeled
#     - iv is an R formula specifying the independent variables in the mydata data frame to use in the model
#     - data specifies the data frame in which the dv and iv variables can be found
#   - Making predictions :
#     - p <- predict(m, test, type = "vector")
#     - m is a model trained by the rpart() function
#     - test is a data frame containing test data with the same features as the training data used to build the model
#     - type specifies the type of prediction to return, either "vector" (for predicted numeric values), "class" for predicted classes, or "prob" (for predicted class probabilities)

m.rpart <- rpart(quality ~ ., data = wine_train)
m.rpart
```

### Visualizing decision trees
```{r}
#install.packages("rpart.plot")

# - Use the rpart.plot() function to produce a tree diagram
library(rpart.plot)
rpart.plot(m.rpart, digits = 3)

# - Some useful options
#   - fallen.leaves parameter forces the leaf nodes to be aligned at bottom of plot
#   - type and extra affects the way the decisions and nodes are labeled
rpart.plot(m.rpart, digits = 4, fallen.leaves = TRUE,
           type = 3, extra = 101)
```

### Step 4 - evaluating model performance
```{r}
p.rpart <- predict(m.rpart, wine_test)

summary(p.rpart)
summary(wine_test$quality)

# - Correlation between the predicted and actual quality values
cor(p.rpart, wine_test$quality)
```

### Measuring performance with the mean absolute error
```{r}
# - The measurement is called the mean absolute error(MAE).
#   - MAE = 1/n*sum(e)
MAE <- function(actual, predicted) {
  mean(abs(actual - predicted))
}

# - The MAE for our predictions is then :
MAE(p.rpart, wine_test$quality)

# - The mean quality rating in the training data is as follows :
mean(wine_train$quality)

MAE(5.87, wine_test$quality)
```

### Step 5 - improving model performance
```{r}
# - Model tree is the Cubist algorithm, an enhancement of the M5 tree algorithm
# - The Cubist algorithm is available in R via the Cubist package
#install.packages("Cubist")

# - Model trees syntax
#   - Building the model
#     - m <- cubist(train, class)
#     - train is a data frame or matrix containing training data
#     - class is a factor vector with class for each row in training data
#   - Making predictions
#     - p <- predict(m, test)
#     - m is a model trained by the cubist() function
#     - test is a data frame containing test data with the same features as the training data used to build the model

library(Cubist)
m.cubist <- cubist(x = wine_train[-12], y = wine_train$quality)
m.cubist

# - Examine the model to see the rules
summary(m.cubist)

# - Examine the performance
p.cubist <- predict(m.cubist, wine_test)
summary(p.cubist)

# - The model slightly reduced the mean absolute error :
MAE(wine_test$quality, p.cubist)
```

