---
Machine Learning with R
Chapter 09 - Finding Groups of Data - Clustering with k-means
---

# Chapter 09 - Finding Groups of Data - Clustering with k-means

  - Clustering : spotting patterns in data.  For example :
    
## Understanding clustering

### Clustering as a machine learning task

  - In clustering, unlabeled examples are given a new cluster label that has been inferred entirely from the relationships within the data.
  - <b>Unsupervised classification</b> : classifies unlabeled examples.
  - Uses boundaries to identify clusters of closely related grouped data points.

### The k-means clustering algorithm

  - The <b>k-means algorithm</b> is perhaps the most often used clusting method.
    - <b>Strengths</b>
      - Uses simple principles that can be explained in non-statistical terms
      - Highly flexible and can be adapted with simple adjustments to address nearly all of its shortcomings
      - Performs well enough under many real-world use cases
    - <b>Weaknesses</b>
      - Not as sophisticated as more modern clustering algorithms
      - Because it uses an element of random change, it is not guaranteed to find the optimal set of clusters
      - Requires a reasonable guess as to how many clusters naturally exist in the data
      - Not ideal for non-spherical clusters or clusters of widely varying density
  - The k-means algorithm assigns each of the n examples to one of the k clusters, where k in a number that has been determined ahead of time.
  - The algorithm uses a heuristic process that finds locally optimal solutions.

### Using distance to assign and update clusters

  - The k-means algorithm begins by choosing k points in the feature space to serve as cluster centers.
  - The k-means algorithm is highly sensitive to starting position of the cluster centers, that means that random chance may have a substantial impact on the final set of clusters.
  - After choosing the initial cluster centers, the other examples are assigned to the cluster center that is nearest according to the distance function.
    - Euclidean distance : dist(x, y) = sqrt(sum(x[i] - y[i]))
  - <b>Voronoi diagram</b> : the dashed lines indicate the boundaries.
  - The first step of udating the clusters involves shifting the initial centers to a new location, known as <b>centroid</b>, which is calculated as the average position of the points currently assigned to that cluster.

### Choosing the appropriate number of clusters

  - k-means is sensitive to the number of clusters; the choice requires a delicate balance.
  - Ideally, you will have <i>a priori</i> knowledge (a prior belief) about the troue groupings and you can apply this information to choosing the number of clusters.
  - Without any prior knowledge, on rule of thumb suggests setting k equal to the square root of (n/2), where n is the number of examples in the data set.
  - <b>Elbow method</b> : attempts to gauge how to homogeneity or heterogeneity within the clusters changes for various values of k.
    - The goal is not to maximize homogeneity or minimize heterogeneity endlessly, but rather to find k such that there are diminishing returns beyond that value.
    - The value of k is known as the <b>elbow point</b>, becuase it looks like an elbow.

## Finding teen market segments using k-means clustering

  - <b>social networking service (SNS)</b> : facebook, tumblr, and instagram.
    = Given the text of teenagers SNS pages, we can identify groups that share common interest such as sports, religion, or music.

### Step 1 - collecting data

  - Using a random sample of 30000 US high school students who had profiles on a well-known SNS in 2006.
  - The SNS is unamed and the users should stay anonymous.

### Step 2 - exploring and preparing the data

```{r}
# read in our data
teens <- read.csv("snsdata.csv")

# Review the data:
str(teens)

# There are a bunch of NA's (missing value) in the gener column
table(teens$gender)

# To see the NA's, specify the useNA
table(teens$gender, useNA = "ifany")

```

#### Data preparation - dummy coding missing values 

```{r}
# - An easy solution for handling missing values is to exclude any record with a missing value.
# - An alternative solution for categorical data like gender is to treat a missing value as a separate category.
# - Dummy Encode
#   - Since, if a record isn't female or NA, then it must be male we don't need to encode for male
teens$female <- ifelse(teens$gender == "F" &
                         !is.na(teens$gender), 1, 0)
teens$no_gender <- ifelse(is.na(teens$gender), 1, 0)

# - Let's compare our constructed dummy variables to the original gender variable:
table(teens$gender, useNA = "ifany")
table(teens$female, useNA = "ifany")
table(teens$no_gender, useNA = "ifany")
```

#### Data preparation - imputing the missing values 

```{r}
# - Lets eliminate the 5,523 missing age values.  Using imputation, which 
#   involves filling the missing data with a guess as to the true value.
# - Typically done using mean / average
mean(teens$age)

# - The issue is that the mean value is undefined for a vector containing
#   missing data.
mean(teens$age, na.rm = TRUE)

# - Calculates the mean age by graduation year after removing the na values:
aggregate(data = teens, age ~ gradyear, mean, na.rm = TRUE)

# - The mean age differs by roughly one year per change in graduation year.
# - Use the ave() function to return a vector with the group means repeated
#   such that the result is equal in length
ave_age <- ave(teens$age, teens$gradyear, FUN = 
                 function(x) mean(x, na.rm = TRUE))

# - Input the means onto the missing values
teens$age <- ifelse(is.na(teens$age), ave_age, teens$age)
summary(teens$age)
```

### Step 3 - training a model on the data

```{r}
# - We will use an implementation of k-means in the stats package
#install.packages("stats")
library(stats)

# - Clustering syntax
#   - Finding clusters:
#     - myclusters <- kmeans(mydata,)
#     - mydata is a matrix or data frame with the examples to be clustered
#     - k specifies the desired number of clusters
#   -Examining clusters:
#     - myclusters$cluster is a vector of cluster assignments from the kmeans() 
#       function
#     - myclusters$centers is a matrix indicating the mean values for each 
#       feature and cluster combination
#     - myclusters$size lists the number of examples assigned to each cluster

# - kmeans() function requires a data frame containing only numeric data and a 
#   parameter specifying the desired number of clusters.
# - Stare our analysis by considering only the 36 features
interests <- teens[5:40]

# - Common practice to normalize or z-score standardize the features such that 
#   each utilizes the same range.
interests_z <- as.data.frame(lapply(interests, scale))

# - Confirm the transformation worked
summary(interests$basketball)
summary(interests_z$basketball)

# - Lastly decide how many clusters to use for segmenting our data.  Start with 5
RNGversion("3.5.2")
set.seed(2345)
teen_clusters <- kmeans(interests_z, 5)
```

### Step 4 - evaluating model performance

```{r}
# - Check the size of the clusters.  If the groups are to large or small, they
#   are not to useful
teen_clusters$size

# - Look at the centers
teen_clusters$centers
```

### Step 5 - improving model performance

```{r}
# - Ally clusters to the full dataset
teens$cluster <-teen_clusters$cluster

# exmine how the cluster assignment relates to individual characteristics.
teens[1:5, c("cluster", "gender", "age", "friends")]

# - Use the aggregate function, look at the demographic characteristics
aggregate(data = teens, age ~ cluster, mean)
aggregate(data = teens, female ~ cluster, mean)
aggregate(data = teens, friends ~ cluster, mean)
```
